[
  {
    "source": "Academic",
    "metadata": {
      "timestamp": "2026-01-30T11:30:00Z",
      "data_source": "official",
      "access_status": "success",
      "quality_score": 88,
      "quality_grade": "A"
    },
    "content": {
      "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
      "authors": "Linhan Wang, Zichong Yang, Chen Bai, Guoxiang Zhang, Xiaotong Liu, Xiaoyin Zheng, Xiao-Xiao Long, Chang-Tien Lu, Cheng Lu",
      "url": "https://arxiv.org/abs/2601.22032v1",
      "arxiv_id": "2601.22032",
      "publish_date": "2026-01-29",
      "category": "cs.CV",
      "innovation": "提出Drive-JEPA框架，集成视频联合嵌入预测架构（V-JEPA）与多模态轨迹蒸馏，通过动量感知选择机制实现稳定安全的端到端驾驶",
      "application_potential": "高",
      "abstract": "端到端自动驾驶越来越多地利用自监督视频预训练来学习可迁移的规划表示。然而，为场景理解预训练视频世界模型目前仅带来有限的改进。这一限制因驾驶固有的模糊性而加剧：每个场景通常只提供单个人类轨迹，使得学习多模态行为变得困难。本文提出Drive-JEPA，一个集成视频联合嵌入预测架构（V-JEPA）与多模态轨迹蒸馏的端到端驾驶框架。首先，我们将V-JEPA适配到端到端驾驶，在大规模驾驶视频上预训练ViT编码器以产生与轨迹规划对齐的预测表示。其次，我们引入基于提议的规划器，将模拟器生成的多轨迹与人类轨迹进行蒸馏，通过动量感知选择机制促进稳定安全的行为。在NAVSIM评估中，V-JEPA表示结合简单的transformer解码器在无感知设置中比先前方法高出3 PDMS。完整的Drive-JEPA框架在v1上达到93.3 PDMS，在v2上达到87.8 EPDMS，创造了新的state-of-the-art。",
      "keywords": ["end-to-end autonomous driving", "video JEPA", "trajectory distillation", "world model", "self-supervised learning"],
      "code_available": false,
      "conference": null,
      "citation_count": null,
      "code_availability": "目前未公开代码（论文刚发布）。预计未来几周内可能在GitHub发布。",
      "reproduction_difficulty": "中高。需要NAVSIM仿真环境、大规模驾驶视频数据集、V-JEPA预训练模型。计算资源需求高（多GPU训练）。方法涉及复杂的多模态蒸馏和动量选择机制。"
    },
    "validation": {
      "url_verified": true,
      "date_verified": true,
      "cross_check": false,
      "issues": []
    }
  },
  {
    "source": "Academic",
    "metadata": {
      "timestamp": "2026-01-30T11:30:00Z",
      "data_source": "official",
      "access_status": "success",
      "quality_score": 85,
      "quality_grade": "A"
    },
    "content": {
      "title": "LLM-Driven Scenario-Aware Planning for Autonomous Driving",
      "authors": "He Li, Zhaowei Chen, Rui Gao, Guoliang Li, Qi Hao, Shuai Wang, Chengzhong Xu",
      "url": "https://arxiv.org/abs/2601.21876v1",
      "arxiv_id": "2601.21876",
      "publish_date": "2026-01-29",
      "category": "cs.RO",
      "innovation": "提出LAP（LLM-Driven Adaptive Planning）方法，利用大型语言模型进行场景理解，在混合规划器切换框架中实现模式配置与运动规划的联合优化，提升密集交通下的规划性能",
      "application_potential": "高",
      "abstract": "自动驾驶的混合规划器切换框架需要在高速驾驶效率与密集交通下的安全操控之间取得平衡。现有的HPSF方法往往因启发式场景识别和低频控制更新而无法进行可靠模式切换或维持高效驾驶。为解决这一限制，本文提出LAP，一种LLM驱动的自适应规划方法，在低复杂度场景中切换到高速驾驶，在高复杂度场景中切换到精确驾驶，通过狭窄间隙生成高质量轨迹。这是通过利用LLM进行场景理解，并将其推理集成到模式配置和运动规划的联合优化中实现的。联合优化通过树搜索模型预测控制和交替最小化求解。我们在机器人操作系统（ROS）中用Python实现LAP。高保真仿真结果表明，所提出的LAP在驾驶时间和成功率方面均优于其他基准方法。",
      "keywords": ["autonomous driving", "LLM planning", "scenario awareness", "hybrid planner", "trajectory generation"],
      "code_available": false,
      "conference": null,
      "citation_count": null,
      "code_availability": "未公开代码。论文提及ROS实现，但未提供GitHub链接。可能需要联系作者获取实现细节。",
      "reproduction_difficulty": "中。需要ROS环境、LLM集成（如GPT-4或开源模型）、自定义仿真环境。方法涉及复杂的联合优化和树搜索MPC，实现难度中等。"
    },
    "validation": {
      "url_verified": true,
      "date_verified": true,
      "cross_check": false,
      "issues": []
    }
  },
  {
    "source": "Academic",
    "metadata": {
      "timestamp": "2026-01-30T11:30:00Z",
      "data_source": "official",
      "access_status": "success",
      "quality_score": 83,
      "quality_grade": "B"
    },
    "content": {
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "authors": "Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen",
      "url": "https://arxiv.org/abs/2601.22054v1",
      "arxiv_id": "2601.22054",
      "publish_date": "2026-01-29",
      "category": "cs.CV, cs.AI",
      "innovation": "提出Metric Anything预训练框架，通过稀疏度量提示从噪声异质3D数据中学习度量深度，首次展示了度量深度领域的扩展定律",
      "application_potential": "高",
      "abstract": "扩展推动了视觉基础模型的近期进展，但将这一范式扩展到度量深度估计仍然具有挑战性，原因包括异质传感器噪声、相机依赖偏差以及噪声跨源3D数据中的度量模糊性。我们引入Metric Anything，一个简单且可扩展的预训练框架，无需人工设计的提示、相机特定建模或任务特定架构，即可从噪声、多样的3D源中学习度量深度。我们方法的核心是稀疏度量提示，通过随机掩码深度图创建，作为通用接口，将空间推理与传感器和相机偏差解耦。使用约2000万图像-深度对，跨越10000个相机模型的重建、捕捉和渲染3D数据，我们首次展示了度量深度领域的明确扩展趋势。预训练模型在深度补全、超分辨率和雷达-相机融合等提示驱动任务中表现出色，而其蒸馏的无提示学生模型在单目深度估计、相机内参恢复、单/多视图度量3D重建和VLA规划方面达到了state-of-the-art结果。我们还表明，使用Metric Anything的预训练ViT作为视觉编码器能显著提升多模态大语言模型的空间智能能力。这些结果表明度量深度估计可以从驱动现代基础模型的相同扩展定律中受益，为可扩展高效的现实世界度量感知开辟了新途径。",
      "keywords": ["metric depth estimation", "foundation models", "pretraining", "3D perception", "autonomous driving perception"],
      "code_available": true,
      "conference": null,
      "citation_count": null,
      "code_availability": "代码已开源：http://metric-anything.github.io/metric-anything-io/。项目页面提供GitHub链接和预训练模型。",
      "reproduction_difficulty": "中低。代码开源且文档齐全。需要大量计算资源进行预训练（约2000万图像-深度对），但推理阶段相对轻量。对于自动驾驶感知任务有直接应用价值。"
    },
    "validation": {
      "url_verified": true,
      "date_verified": true,
      "cross_check": false,
      "issues": []
    }
  },
  {
    "source": "Academic",
    "metadata": {
      "timestamp": "2026-01-30T11:30:00Z",
      "data_source": "official",
      "access_status": "success",
      "quality_score": 82,
      "quality_grade": "B"
    },
    "content": {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "authors": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu",
      "url": "https://arxiv.org/abs/2601.22153v1",
      "arxiv_id": "2601.22153",
      "publish_date": "2026-01-29",
      "category": "cs.RO, cs.CV",
      "innovation": "提出DynamicVLA框架，通过连续推理和潜在感知动作流式传输实现动态物体操作，填补动态操作数据空白",
      "application_potential": "中高",
      "abstract": "动态物体操作对于视觉-语言-动作（VLA）模型仍然是一个开放挑战，这些模型在静态操作中表现出强大的泛化能力，但在需要快速感知、时序预测和连续控制的动态场景中却表现不佳。我们提出DynamicVLA，一个用于动态物体操作的框架，通过三个关键设计集成时序推理和闭环适应：1）使用卷积视觉编码器构建紧凑的0.4B VLA，实现空间高效、结构忠实的编码，支持快速多模态推理；2）连续推理，实现重叠推理和执行，降低延迟并及时适应物体运动；3）潜在感知动作流式传输，通过强制执行时序对齐的动作执行来弥合感知-执行差距。为了填补动态操作数据的空白，我们引入动态物体操作（DOM）基准，通过自动数据收集管道从头构建，高效收集跨2.8K场景和206个物体的200K合成片段，并支持无需遥操作的2K真实世界片段快速收集。广泛的评估证明了在响应速度、感知和泛化方面的显著改进，将DynamicVLA定位为跨实现方式的通用动态物体操作统一框架。",
      "keywords": ["VLA model", "dynamic manipulation", "robotics", "continuous inference", "autonomous systems"],
      "code_available": true,
      "conference": null,
      "citation_count": null,
      "code_availability": "代码已开源：GitHub链接 https://github.com/hzxie/DynamicVLA。项目页面提供完整代码和DOM基准。",
      "reproduction_difficulty": "中。需要机器人仿真环境（如Isaac Sim）、VLA模型训练基础设施。方法相对新颖但代码完整。对于自动驾驶中动态物体交互有启发意义。"
    },
    "validation": {
      "url_verified": true,
      "date_verified": true,
      "cross_check": false,
      "issues": []
    }
  },
  {
    "source": "Academic",
    "metadata": {
      "timestamp": "2026-01-30T11:30:00Z",
      "data_source": "official",
      "access_status": "success",
      "quality_score": 80,
      "quality_grade": "B"
    },
    "content": {
      "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
      "authors": "Johann Christensen, Elena Hoemann, Frank Köster, Sven Hallerbach",
      "url": "https://arxiv.org/abs/2601.22118v1",
      "arxiv_id": "2601.22118",
      "publish_date": "2026-01-29",
      "category": "cs.AI",
      "innovation": "提出基于数据的安全关键AI系统运行条件定义方法，使用多维核表示从数据中后验定义ODD（运行设计域），为数据驱动的安全关键系统认证提供新途径",
      "application_potential": "高",
      "abstract": "人工智能（AI）在许多领域日益普及，包括众多安全关键应用。然而，对于现实世界中的复杂系统或现有数据，定义底层环境条件极具挑战性。这往往导致对AI系统必须运行的环境描述不完整。然而，这种描述称为运行设计域（ODD），在许多领域中对于基于AI系统的认证是必需的。传统上，ODD在开发过程的早期创建，利用复杂的专家知识和相关标准。本文提出一种新颖的安全设计方法，使用多维核表示从先前收集的数据中后验定义ODD。该方法通过蒙特卡洛方法和真实世界航空用例（用于未来安全关键防撞系统）进行验证。此外，通过定义两个ODD相等的条件，论文表明数据驱动的ODD可以等于数据的原始、隐藏底层ODD。利用新颖的、基于安全设计的核基ODD可以实现未来数据驱动的、安全关键的基于AI系统的认证。",
      "keywords": ["safety-critical AI", "operational design domain", "certification", "autonomous driving safety", "AI safety"],
      "code_available": false,
      "conference": null,
      "citation_count": null,
      "code_availability": "未公开代码。论文提出理论框架和数学方法，但未提供实现代码。",
      "reproduction_difficulty": "中高。需要深入理解核方法和ODD概念。实现涉及多维核密度估计和ODD等价性验证。对于自动驾驶安全认证有重要应用价值。"
    },
    "validation": {
      "url_verified": true,
      "date_verified": true,
      "cross_check": false,
      "issues": []
    }
  }
]